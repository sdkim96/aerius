{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 06:03:00.298638: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-17 06:03:00.366820: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-17 06:03:00.368906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 06:03:01.733601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Preprocessed Queries:\n",
      "[[('안녕하세요', 'NNP')], [('오늘', 'NNG'), ('날씨', 'NNP'), ('어떻', 'VA'), ('어요', 'EF'), ('?', 'SF')], []]\n",
      "Extracted Words:\n",
      "['안녕하세요', '오늘', '날씨', '어떻', '어요', '?']\n",
      "Converted Sequences:\n",
      "[[1], [2, 3, 4, 5], []]\n",
      "Converted Sequences:\n",
      "[[1], [2, 3, 4, 5], []]\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import *\n",
    "\n",
    "# Sample queries\n",
    "queries = ['안녕하세요', '오늘 날씨 어때요?', '...']\n",
    "\n",
    "p = Preprocess()\n",
    "\n",
    "# Step 1: Preprocess the sentences\n",
    "preprocessed_queries = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        preprocessed_queries.append(preprocessed)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# Print the preprocessed queries\n",
    "print(\"Preprocessed Queries:\")\n",
    "print(preprocessed_queries)\n",
    "\n",
    "# Step 2: Extract words from the preprocessed queries\n",
    "words = []\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    words.extend(word_list)\n",
    "\n",
    "# Print the extracted words\n",
    "print(\"Extracted Words:\")\n",
    "print(words)\n",
    "\n",
    "# Step 3: Initialize the tokenizer with the extracted words\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Step 4: Convert the queries into sequences\n",
    "sequences = []\n",
    "# for sentence in queries:\n",
    "#     sequence = p.text_to_sequence(sentence)\n",
    "#     sequences.append(sequence)\n",
    "\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    sequence = p.text_to_sequence(' '.join(word_list))  # Join the words to form the sentence\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Print the converted sequences\n",
    "print(\"Converted Sequences:\")\n",
    "print(sequences)\n",
    "\n",
    "# Print the converted sequences\n",
    "print(\"Converted Sequences:\")\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of '어떻': 4\n",
      "Index of '어요': 5\n",
      "Original Sentence: 안녕하세요\n",
      "Converted Sequence: [1]\n",
      "Original Sentence: 오늘 날씨 어때요?\n",
      "Converted Sequence: [2, 3]\n",
      "Original Sentence: ...\n",
      "Converted Sequence: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of '어떻':\", p.tokenizer.word_index['어떻'])\n",
    "print(\"Index of '어요':\", p.tokenizer.word_index['어요'])\n",
    "\n",
    "# Debugging the text_to_sequence function\n",
    "for sentence in queries:\n",
    "    print(\"Original Sentence:\", sentence)\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    print(\"Converted Sequence:\", sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "After delete_intent_trash_tags: [('안녕하세요', 'NNP')]\n",
      "Words List: ['안녕하세요', '오늘', '날씨', '어떻', '어요', '?']\n",
      "Words List Length: 6\n",
      "Word Index: {'안녕하세요': 1, '오늘': 2, '날씨': 3, '어떻': 4, '어요': 5}\n",
      "Original Sentence: 안녕하세요\n",
      "Converted Sequence: [1]\n",
      "Original Sentence: 오늘 날씨 어때요?\n",
      "Converted Sequence: [2, 3]\n",
      "Original Sentence: ...\n",
      "Converted Sequence: []\n",
      "Converted Sequences: [[1], [2, 3], []]\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# Preprocessing 객체 초기화\n",
    "p1 = Preprocess()\n",
    "\n",
    "# 첫 번째 문장 가져와서 처리\n",
    "sentence = queries[0]\n",
    "preprocessed = p1.delete_intent_trash_tags(sentence=sentence)\n",
    "print(\"After delete_intent_trash_tags:\", preprocessed)\n",
    "\n",
    "# 단어 리스트 생성\n",
    "words = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p1.delete_intent_trash_tags(sentence=sentence)\n",
    "        word_list, _ = p1.divide_words_tags(preprocessed)\n",
    "        words.extend(word_list)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# words 리스트 확인\n",
    "print(\"Words List:\", words[:30])  # 첫 30개의 단어만 출력\n",
    "print(\"Words List Length:\", len(words))  # words 리스트의 길이 출력\n",
    "\n",
    "# 토크나이저 초기화\n",
    "p1.initialize_tokenizer(words)\n",
    "\n",
    "# 토크나이저의 단어사전 확인\n",
    "print(\"Word Index:\", p1.tokenizer.word_index)\n",
    "\n",
    "# queries의 각 문장을 시퀀스로 변환\n",
    "sequences = []\n",
    "for sentence1 in queries:\n",
    "    sequence = p1.text_to_sequence(sentence1)\n",
    "    sequences.append(sequence)\n",
    "    print(\"Original Sentence:\", sentence1)\n",
    "    print(\"Converted Sequence:\", sequence)\n",
    "\n",
    "# 변환된 시퀀스 확인\n",
    "print(\"Converted Sequences:\", sequences[:20])  # 첫 20개의 시퀀스만 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

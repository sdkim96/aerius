{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query    intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?  주문_취소_확인      7\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?  주문_취소_확인      7\n",
      "2                     택배비 따로 추가되나요?  배송_비용_질문      5\n",
      "3                          택배비 있나요?  배송_비용_질문      5\n",
      "4                        택배비 따로 들어요  배송_비용_질문      5\n",
      "len of queries =  203069\n",
      "len of intents =  203069\n"
     ]
    }
   ],
   "source": [
    "filename='intents_labeled_by_9_first_try'\n",
    "\n",
    "train_file = f'/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/{filename}.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    30947\n",
       "1    30947\n",
       "6    30947\n",
       "4    30947\n",
       "5    30947\n",
       "8    30947\n",
       "7    30947\n",
       "3    30947\n",
       "2    30947\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data.sample(frac=0.7, random_state=42)\n",
    "temp_data = data.drop(train_data.index)\n",
    "val_data = temp_data.sample(frac=0.66, random_state=42)\n",
    "test_data = temp_data.drop(val_data.index)\n",
    "\n",
    "# Oversampling the minority classes in training data\n",
    "max_size = train_data['label'].value_counts().max()\n",
    "lst = [train_data]\n",
    "for class_index, group in train_data.groupby('label'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "train_data_oversampled = pd.concat(lst)\n",
    "\n",
    "# Checking the distribution after oversampling\n",
    "oversampled_distribution = train_data_oversampled['label'].value_counts()\n",
    "\n",
    "oversampled_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     query    intent  label\n",
      "89804                     M싸이는 판매 예정 없으세요?  제품_재고_질문      0\n",
      "119566                   31 A 원단이 무겁지 않나요?  제품_정보_질문      1\n",
      "134364    특대는 허리가 32 이상 되는 남자가 입어야 되는 건가여?  제품_정보_질문      1\n",
      "181223  마스크 트랩 택 제거 하고 싶은데, 혹시 여기서도 가능한가요?  매장_이용_질문      6\n",
      "3797             90B 에 팬티는 100으로 구매할수 없나요?  제품_구성_질문      4\n",
      "len of queries =  278523\n",
      "len of intents =  278523\n"
     ]
    }
   ],
   "source": [
    "queries = train_data_oversampled['query'].tolist()\n",
    "intents = train_data_oversampled['label'].tolist()\n",
    "\n",
    "print(train_data_oversampled.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 9\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess()\n",
    "\n",
    "words = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        word_list, _ = p.divide_words_tags(preprocessed)\n",
    "        words.extend(word_list)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Convert the queries into sequences\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9749/9749 [==============================] - 225s 23ms/step - loss: 1.5134 - accuracy: 0.4735 - val_loss: 1.2831 - val_accuracy: 0.5499\n",
      "Epoch 2/5\n",
      "9749/9749 [==============================] - 264s 27ms/step - loss: 1.3361 - accuracy: 0.5363 - val_loss: 1.1823 - val_accuracy: 0.5834\n",
      "Epoch 3/5\n",
      "9749/9749 [==============================] - 272s 28ms/step - loss: 1.2734 - accuracy: 0.5564 - val_loss: 1.1283 - val_accuracy: 0.6019\n",
      "Epoch 4/5\n",
      "9749/9749 [==============================] - 299s 31ms/step - loss: 1.2375 - accuracy: 0.5688 - val_loss: 1.0917 - val_accuracy: 0.6131\n",
      "Epoch 5/5\n",
      "9749/9749 [==============================] - 301s 31ms/step - loss: 1.2089 - accuracy: 0.5786 - val_loss: 1.0621 - val_accuracy: 0.6270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f94a98fc0d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 128,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 128,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128,\n",
    "               kernel_size = 5,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393/1393 [==============================] - 23s 12ms/step\n",
      "Precision: 0.1145\n",
      "Recall: 0.1142\n",
      "F1 Score: 0.1120\n",
      "Accuracy: 0.1142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.065037727355957\n",
      "Test accuracy: 0.6251256465911865\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393/1393 [==============================] - 17s 11ms/step - loss: 1.0693 - accuracy: 0.6241\n",
      "Accuracy: 62.41203546524048\n",
      "loss: 1.0693109035491943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projects/aerius/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/{filename}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

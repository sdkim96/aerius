{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?     주문    0.0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?     주문    0.0\n",
      "2                     택배비 따로 추가되나요?     배송    1.0\n",
      "3                          택배비 있나요?     배송    1.0\n",
      "4                        택배비 따로 들어요     배송    1.0\n",
      "len of queries =  79617\n",
      "len of intents =  79617\n"
     ]
    }
   ],
   "source": [
    "filename='230817_intent_labeled_by_6'\n",
    "\n",
    "train_file = f'/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/{filename}.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1.0    24399\n",
       "3.0    24399\n",
       "4.0    24399\n",
       "2.0    24399\n",
       "5.0    24399\n",
       "0.0    24399\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data.sample(frac=0.7, random_state=42)\n",
    "temp_data = data.drop(train_data.index)\n",
    "val_data = temp_data.sample(frac=0.66, random_state=42)\n",
    "test_data = temp_data.drop(val_data.index)\n",
    "\n",
    "# Oversampling the minority classes in training data\n",
    "max_size = train_data['label'].value_counts().max()\n",
    "lst = [train_data]\n",
    "for class_index, group in train_data.groupby('label'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "train_data_oversampled = pd.concat(lst)\n",
    "\n",
    "# Checking the distribution after oversampling\n",
    "oversampled_distribution = train_data_oversampled['label'].value_counts()\n",
    "\n",
    "oversampled_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   query intent  label\n",
      "31684  3만원 이상 구매해서 오늘 상품받았는데요. 3만원 이상 구매 시 슬리브리스 주신다고...     배송    1.0\n",
      "5771                                          언제쯤 반품됩니까?     AS    3.0\n",
      "78326                                   모델이 입은 사이즈 몇이예요?     제품    4.0\n",
      "44699           추후 카드 취소와 관련된 사항 진행될 때 따로 안내해 주시는 게 있나요?     AS    3.0\n",
      "57292                   어제 산 슬랙스 그린색으로 교환하려면 얼마 더 내야 해요?     AS    3.0\n",
      "len of queries =  146394\n",
      "len of intents =  146394\n"
     ]
    }
   ],
   "source": [
    "queries = train_data_oversampled['query'].tolist()\n",
    "intents = train_data_oversampled['label'].tolist()\n",
    "\n",
    "print(train_data_oversampled.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 6\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# # Preprocessing 객체 초기화\n",
    "# p = Preprocess()\n",
    "\n",
    "# # 첫 번째 문장 가져와서 처리\n",
    "# sentence = queries[0]\n",
    "# preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "# print(\"After delete_intent_trash_tags:\", preprocessed)\n",
    "\n",
    "# # 단어 리스트 생성\n",
    "# words = []\n",
    "# for sentence in queries:\n",
    "#     if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "#         preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "#         word_list, _ = p.divide_words_tags(preprocessed)\n",
    "#         words.extend(word_list)\n",
    "#     else:\n",
    "#         print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# # words 리스트 확인\n",
    "# print(\"Words List:\", words[:30])  # 첫 30개의 단어만 출력\n",
    "# print(\"Words List Length:\", len(words))  # words 리스트의 길이 출력\n",
    "\n",
    "# # 토크나이저 초기화\n",
    "# p.initialize_tokenizer(words)\n",
    "\n",
    "# # 토크나이저의 단어사전 확인\n",
    "# print(\"Word Index:\", p.tokenizer.word_index)\n",
    "\n",
    "# # queries의 각 문장을 시퀀스로 변환\n",
    "# sequences = []\n",
    "# for sentence1 in queries:\n",
    "#     sequence = p.text_to_sequence(sentence1)\n",
    "#     sequences.append(sequence)\n",
    "#     print(\"Original Sentence:\", sentence1)\n",
    "#     print(\"Converted Sequence:\", sequence)\n",
    "\n",
    "# # 변환된 시퀀스 확인\n",
    "# print(\"Converted Sequences:\", sequences[:20])  # 첫 20개의 시퀀스만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Preprocessed Queries:\n",
      "[[('3', 'SN'), ('만원', 'NNP'), ('이상', 'NNG'), ('구매', 'NNG'), ('아서', 'EC'), ('오늘', 'NNG'), ('상품', 'NNG'), ('받', 'VV'), ('았', 'EP'), ('는데요', 'EF'), ('.', 'SF'), ('3', 'SN'), ('만원', 'NNP'), ('이상', 'NNG'), ('구매', 'NNG'), ('시', 'NNB'), ('슬리브리스', 'NA'), ('주', 'VX'), ('시', 'EP'), ('ㄴ다고', 'EC'), ('쓰', 'VV'), ('어', 'EC'), ('지', 'VX'), ('어', 'EC'), ('있', 'VV'), ('는데', 'EC'), ('왜', 'MAG'), ('저', 'NP'), ('안', 'MAG'), ('오', 'VV'), ('걸', 'VV'), ('ㄴ가요', 'EF'), ('?', 'SF')], [('언제', 'NP'), ('쯤', 'NNB'), ('반품', 'NNG'), ('ㅂ니까', 'EF'), ('?', 'SF')], [('모델', 'NNG'), ('입', 'VV'), ('사이즈', 'NNG'), ('몇', 'NR'), ('이', 'VCP'), ('예요', 'EF'), ('?', 'SF')], [('추후', 'NNG'), ('카드', 'NNG'), ('취소', 'NNP'), ('관련', 'NNG'), ('사항', 'NNG'), ('진행', 'NNG'), ('때', 'NNG'), ('따로', 'MAG'), ('안내', 'NNG'), ('아', 'EC'), ('주시', 'NNP'), ('게', 'NNG'), ('있', 'VX'), ('나요', 'EF'), ('?', 'SF')], [('어제', 'MAG'), ('살', 'VV'), ('슬랙스', 'NA'), ('그린', 'NNP'), ('색', 'NNG'), ('교환', 'NNG'), ('려면', 'EC'), ('얼마', 'NNG'), ('더', 'MAG'), ('내야', 'NNP'), ('하', 'VX'), ('아요', 'EF'), ('?', 'SF')]]\n",
      "Extracted Words:\n",
      "['3', '만원', '이상', '구매', '아서']\n",
      "Converted Sequences:\n",
      "[[95, 979, 282, 47, 52, 65, 34, 32, 7, 109, 95, 979, 282, 47, 11, 8638, 15, 11, 234, 210, 10, 43, 10, 2, 4, 55, 181, 20, 37, 33, 12], [17, 85, 45, 110], [296, 35, 5, 74, 3, 206], [4599, 531, 72, 1525, 872, 494, 148, 164, 604, 23, 424, 24, 2, 1], [123, 137, 505, 1132, 189, 36, 218, 168, 82, 458, 9, 62]]\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# Step 1: Initialize Preprocessing object\n",
    "p = Preprocess()\n",
    "\n",
    "# Step 2: Preprocess the sentences\n",
    "preprocessed_queries = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        preprocessed_queries.append(preprocessed)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# Print the preprocessed queries for debugging\n",
    "print(\"Preprocessed Queries:\")\n",
    "print(preprocessed_queries[:5])\n",
    "\n",
    "# Step 3: Extract words from the preprocessed queries\n",
    "words = []\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    words.extend(word_list)\n",
    "\n",
    "# Print the extracted words for debugging\n",
    "print(\"Extracted Words:\")\n",
    "print(words[:5])\n",
    "\n",
    "# Step 4: Initialize the tokenizer with the extracted words\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Step 5: Convert the preprocessed queries into sequences\n",
    "sequences = []\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    sentence = ' '.join(word_list)  # Join the words to form a sentence\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Print the converted sequences for debugging\n",
    "print(\"Converted Sequences:\")\n",
    "print(sequences[:5])\n",
    "\n",
    "# Print the tokenizer's word index for debugging\n",
    "# print(\"Word Index:\")\n",
    "# print(p.tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '만원', '이상', '구매', '아서', '오늘', '상품', '받', '았', '는데요', '.', '3', '만원', '이상', '구매', '시', '슬리브리스', '주', '시', 'ㄴ다고', '쓰', '어', '지', '어', '있', '는데', '왜', '저', '안', '오']\n",
      "[[95, 979, 282, 47, 52, 65, 34, 32, 7, 109, 95, 979, 282, 47, 11, 8638, 15, 11, 234, 210, 10, 43, 10, 2, 4, 55, 181, 20, 37, 33, 12], [17, 85, 45, 110], [296, 35, 5, 74, 3, 206], [4599, 531, 72, 1525, 872, 494, 148, 164, 604, 23, 424, 24, 2, 1], [123, 137, 505, 1132, 189, 36, 218, 168, 82, 458, 9, 62], [8639, 47, 7, 4, 629, 88, 36, 23, 15, 11, 19], [65, 6, 29, 17, 32, 14, 2, 43, 10], [989, 5, 552, 25], [17, 157, 60, 62], [1751, 10, 60, 495, 661, 23, 15, 19], [990, 1428, 5, 74, 269, 61, 1], [958, 1443, 702, 6, 70, 18, 27, 103, 49, 1382, 49, 3, 838, 16, 26, 17, 18, 27, 1], [123, 6, 7, 4, 94, 17, 9, 23, 15, 11, 110], [182, 228, 54, 6, 3, 44, 70, 94, 103, 49, 3324, 17, 42, 10, 835, 33, 12], [157, 17, 8, 33, 12], [94, 288, 263, 10, 43, 48], [34, 14, 51, 9, 23, 60, 11, 26, 4, 1975, 166, 16, 13, 67, 124, 20, 8, 10, 2, 557, 53, 24, 124, 21, 25], [335, 179, 16, 1], [3406, 82, 30, 279, 562, 182, 1053, 3, 151, 281, 336, 22, 2160, 97, 25], [95, 22, 34, 80, 5, 6, 7, 4, 122, 64, 52, 42, 10, 15, 11, 14, 2, 1]]\n",
      "[247, 17, 215, 14, 2, 43]\n"
     ]
    }
   ],
   "source": [
    "print(words[:30])\n",
    "print(sequences[:20])\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = p.tokenizer.to_json()\n",
    "with open(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/tokenizers/{filename}_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247, 17, 215, 14, 2, 43]\n"
     ]
    }
   ],
   "source": [
    "print(sequence[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5124/5124 [==============================] - 130s 24ms/step - loss: 0.3725 - accuracy: 0.8748 - val_loss: 0.1541 - val_accuracy: 0.9495\n",
      "Epoch 2/5\n",
      "5124/5124 [==============================] - 138s 27ms/step - loss: 0.1977 - accuracy: 0.9354 - val_loss: 0.1130 - val_accuracy: 0.9616\n",
      "Epoch 3/5\n",
      "5124/5124 [==============================] - 147s 29ms/step - loss: 0.1567 - accuracy: 0.9487 - val_loss: 0.0952 - val_accuracy: 0.9660\n",
      "Epoch 4/5\n",
      "5124/5124 [==============================] - 158s 31ms/step - loss: 0.1344 - accuracy: 0.9556 - val_loss: 0.0785 - val_accuracy: 0.9726\n",
      "Epoch 5/5\n",
      "5124/5124 [==============================] - 149s 29ms/step - loss: 0.1182 - accuracy: 0.9603 - val_loss: 0.0671 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb6ba636fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 128,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 128,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128,\n",
    "               kernel_size = 5,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732/732 [==============================] - 11s 9ms/step\n",
      "Precision: 0.1658\n",
      "Recall: 0.1657\n",
      "F1 Score: 0.1657\n",
      "Accuracy: 0.1657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07064834982156754\n",
      "Test accuracy: 0.9763644933700562\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732/732 [==============================] - 6s 8ms/step - loss: 0.0653 - accuracy: 0.9768\n",
      "Accuracy: 97.68427014350891\n",
      "loss: 0.06529298424720764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projects/aerius/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/models/{filename}_oversampling.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

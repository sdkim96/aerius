{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?     주문    0.0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?     주문    0.0\n",
      "2                     택배비 따로 추가되나요?     배송    1.0\n",
      "3                          택배비 있나요?     배송    1.0\n",
      "4                        택배비 따로 들어요     배송    1.0\n",
      "len of queries =  79617\n",
      "len of intents =  79617\n"
     ]
    }
   ],
   "source": [
    "filename='230817_intent_labeled_by_6'\n",
    "\n",
    "train_file = f'/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/{filename}.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = data.sample(frac=0.7, random_state=42)\n",
    "# temp_data = data.drop(train_data.index)\n",
    "# val_data = temp_data.sample(frac=0.66, random_state=42)\n",
    "# test_data = temp_data.drop(val_data.index)\n",
    "\n",
    "# # Oversampling the minority classes in training data\n",
    "# max_size = train_data['label'].value_counts().max()\n",
    "# lst = [train_data]\n",
    "# for class_index, group in train_data.groupby('label'):\n",
    "#     lst.append(group.sample(max_size-len(group), replace=True))\n",
    "# train_data_oversampled = pd.concat(lst)\n",
    "\n",
    "# # Checking the distribution after oversampling\n",
    "# oversampled_distribution = train_data_oversampled['label'].value_counts()\n",
    "\n",
    "# oversampled_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?     주문    0.0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?     주문    0.0\n",
      "2                     택배비 따로 추가되나요?     배송    1.0\n",
      "3                          택배비 있나요?     배송    1.0\n",
      "4                        택배비 따로 들어요     배송    1.0\n",
      "len of queries =  79617\n",
      "len of intents =  79617\n"
     ]
    }
   ],
   "source": [
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 6\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# # Preprocessing 객체 초기화\n",
    "# p = Preprocess()\n",
    "\n",
    "# # 첫 번째 문장 가져와서 처리\n",
    "# sentence = queries[0]\n",
    "# preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "# print(\"After delete_intent_trash_tags:\", preprocessed)\n",
    "\n",
    "# # 단어 리스트 생성\n",
    "# words = []\n",
    "# for sentence in queries:\n",
    "#     if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "#         preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "#         word_list, _ = p.divide_words_tags(preprocessed)\n",
    "#         words.extend(word_list)\n",
    "#     else:\n",
    "#         print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# # words 리스트 확인\n",
    "# print(\"Words List:\", words[:30])  # 첫 30개의 단어만 출력\n",
    "# print(\"Words List Length:\", len(words))  # words 리스트의 길이 출력\n",
    "\n",
    "# # 토크나이저 초기화\n",
    "# p.initialize_tokenizer(words)\n",
    "\n",
    "# # 토크나이저의 단어사전 확인\n",
    "# print(\"Word Index:\", p.tokenizer.word_index)\n",
    "\n",
    "# # queries의 각 문장을 시퀀스로 변환\n",
    "# sequences = []\n",
    "# for sentence1 in queries:\n",
    "#     sequence = p.text_to_sequence(sentence1)\n",
    "#     sequences.append(sequence)\n",
    "#     print(\"Original Sentence:\", sentence1)\n",
    "#     print(\"Converted Sequence:\", sequence)\n",
    "\n",
    "# # 변환된 시퀀스 확인\n",
    "# print(\"Converted Sequences:\", sequences[:20])  # 첫 20개의 시퀀스만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Preprocessed Queries:\n",
      "[[('아침', 'NNG'), ('아니', 'VCN'), ('고', 'EC'), ('밤', 'NNG'), ('12', 'SN'), ('시', 'NNB'), ('30', 'SN'), ('분', 'NNB'), ('결제', 'NNG'), ('걸', 'VV'), ('ㄴ데', 'EC'), ('그렇', 'VA'), ('ㄴ가요', 'EF'), ('?', 'SF')], [('실수', 'NNG'), ('취소', 'NNG'), ('면', 'EC'), ('재', 'XPN'), ('주문', 'NNG'), ('아야', 'EC'), ('하', 'VV'), ('거', 'NNB'), ('죠', 'EF'), ('?', 'SF')], [('택배', 'NNP'), ('비', 'NNG'), ('따로', 'MAG'), ('추가', 'NNG'), ('나요', 'EF'), ('?', 'SF')], [('택배', 'NNP'), ('비', 'NNG'), ('있', 'VX'), ('나요', 'EF'), ('?', 'SF')], [('택배', 'NNP'), ('비', 'NNG'), ('따로', 'MAG'), ('들', 'VV'), ('어요', 'EC')]]\n",
      "Extracted Words:\n",
      "['아침', '아니', '고', '밤', '12']\n",
      "Converted Sequences:\n",
      "[[613, 71, 17, 681, 281, 13, 264, 399, 122, 30, 44, 271, 14], [682, 143, 28, 236, 19, 64, 11, 23, 26], [52, 91, 192, 258, 1], [52, 91, 4, 1], [52, 91, 192, 145, 24]]\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# Step 1: Initialize Preprocessing object\n",
    "p = Preprocess()\n",
    "\n",
    "# Step 2: Preprocess the sentences\n",
    "preprocessed_queries = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        preprocessed_queries.append(preprocessed)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "# Print the preprocessed queries for debugging\n",
    "print(\"Preprocessed Queries:\")\n",
    "print(preprocessed_queries[:5])\n",
    "\n",
    "# Step 3: Extract words from the preprocessed queries\n",
    "words = []\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    words.extend(word_list)\n",
    "\n",
    "# Print the extracted words for debugging\n",
    "print(\"Extracted Words:\")\n",
    "print(words[:5])\n",
    "\n",
    "# Step 4: Initialize the tokenizer with the extracted words\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Step 5: Convert the preprocessed queries into sequences\n",
    "sequences = []\n",
    "for preprocessed in preprocessed_queries:\n",
    "    word_list, _ = p.divide_words_tags(preprocessed)\n",
    "    sentence = ' '.join(word_list)  # Join the words to form a sentence\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Print the converted sequences for debugging\n",
    "print(\"Converted Sequences:\")\n",
    "print(sequences[:5])\n",
    "\n",
    "# Print the tokenizer's word index for debugging\n",
    "# print(\"Word Index:\")\n",
    "# print(p.tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아침', '아니', '고', '밤', '12', '시', '30', '분', '결제', '걸', 'ㄴ데', '그렇', 'ㄴ가요', '?', '실수', '취소', '면', '재', '주문', '아야', '하', '거', '죠', '?', '택배', '비', '따로', '추가', '나요', '?']\n",
      "[[613, 71, 17, 681, 281, 13, 264, 399, 122, 30, 44, 271, 14], [682, 143, 28, 236, 19, 64, 11, 23, 26], [52, 91, 192, 258, 1], [52, 91, 4, 1], [52, 91, 192, 145, 24], [52, 91, 18, 1], [52, 91, 299, 111], [5, 8, 159, 4, 1], [15, 850, 112, 43, 111], [170, 5, 8, 2, 26], [5, 8, 91, 112], [21, 5, 8, 91, 111], [5, 8, 91, 1080, 101, 11, 1], [52, 91, 62, 29, 9, 1], [1030, 5, 8, 91, 112, 2, 14], [1602, 751, 175, 32, 25, 3, 5, 8, 91, 94, 299, 9, 1], [19, 29, 9, 28, 192, 5, 1241, 4, 1], [5, 8, 91, 112, 2, 463], [136, 13, 155, 2665, 80, 23, 2, 28, 860, 156, 271, 29, 140, 21, 16, 101, 11, 23, 71, 14], [1223, 179, 170, 5, 8, 2, 26]]\n",
      "[7089, 40, 290, 10, 259, 3100, 1367, 583, 12870, 40, 347, 10, 259, 3100, 1367, 124, 15, 4, 46]\n"
     ]
    }
   ],
   "source": [
    "print(words[:30])\n",
    "print(sequences[:20])\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = p.tokenizer.to_json()\n",
    "with open(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/tokenizers/{filename}_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7089, 40, 290, 10, 259, 3100, 1367, 583, 12870, 40, 347, 10, 259, 3100, 1367, 124, 15, 4, 46]\n"
     ]
    }
   ],
   "source": [
    "print(sequence[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2787/2787 [==============================] - 73s 25ms/step - loss: 0.4241 - accuracy: 0.8550 - val_loss: 0.2520 - val_accuracy: 0.9114\n",
      "Epoch 2/5\n",
      "2787/2787 [==============================] - 80s 29ms/step - loss: 0.2903 - accuracy: 0.9032 - val_loss: 0.2098 - val_accuracy: 0.9251\n",
      "Epoch 3/5\n",
      "2787/2787 [==============================] - 85s 31ms/step - loss: 0.2547 - accuracy: 0.9129 - val_loss: 0.1834 - val_accuracy: 0.9361\n",
      "Epoch 4/5\n",
      "2787/2787 [==============================] - 85s 31ms/step - loss: 0.2352 - accuracy: 0.9191 - val_loss: 0.1588 - val_accuracy: 0.9423\n",
      "Epoch 5/5\n",
      "2787/2787 [==============================] - 92s 33ms/step - loss: 0.2163 - accuracy: 0.9265 - val_loss: 0.1504 - val_accuracy: 0.9476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0b9a10a6d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 128,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 128,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128,\n",
    "               kernel_size = 5,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 6ms/step\n",
      "Precision: 0.3027\n",
      "Recall: 0.3075\n",
      "F1 Score: 0.3050\n",
      "Accuracy: 0.3075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.15340444445610046\n",
      "Test accuracy: 0.9452329874038696\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 7ms/step - loss: 0.1416 - accuracy: 0.9509\n",
      "Accuracy: 95.08855938911438\n",
      "loss: 0.14161065220832825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projects/aerius/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/{filename}_3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스 가중치만 둠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 01:46:17.990291: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 01:46:18.049684: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 01:46:18.051231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 01:46:18.995217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tb_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query  intent\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?       0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?       0\n",
      "2                     택배비 따로 추가되나요?       1\n",
      "3                          택배비 있나요?       1\n",
      "4                        택배비 따로 들어요       1\n",
      "len of queries =  235867\n",
      "len of intents =  235867\n"
     ]
    }
   ],
   "source": [
    "train_file = '/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/intent5.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 4\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess()\n",
    "\n",
    "words = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        word_list, _ = p.divide_words_tags(preprocessed)\n",
    "        words.extend(word_list)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Convert the queries into sequences\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length: 2.434460946211212\n",
      "Std Length: 1.9924301606156376\n",
      "Average + 1x Std: 4\n",
      "Average + 2x Std: 6\n",
      "95th Percentile Length: 6.0\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# 평균과 표준편차 계산\n",
    "average_length = np.mean(sequence_lengths)\n",
    "std_length = np.std(sequence_lengths)\n",
    "\n",
    "# 평균 + 1x or 2x 표준편차\n",
    "max_seq_length_1std = int(average_length + std_length)\n",
    "max_seq_length_2std = int(average_length + 2*std_length)\n",
    "\n",
    "# 문장 길이의 분포 확인\n",
    "percentile_95 = np.percentile(sequence_lengths, 95)\n",
    "\n",
    "print(f\"Average Length: {average_length}\")\n",
    "print(f\"Std Length: {std_length}\")\n",
    "print(f\"Average + 1x Std: {max_seq_length_1std}\")\n",
    "print(f\"Average + 2x Std: {max_seq_length_2std}\")\n",
    "print(f\"95th Percentile Length: {percentile_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SMOTE 오버샘플링 추가 시작\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(padded_seqs, encoded_intents)\n",
    "\n",
    "# ds_resampled = tf.data.Dataset.from_tensor_slices((X_resampled, y_resampled))\n",
    "# ds_resampled = ds_resampled.shuffle(len(X_resampled))\n",
    "\n",
    "# train_size_resampled = int(len(X_resampled)*0.7)\n",
    "# val_size_resampled = int(len(X_resampled)*0.2)\n",
    "# test_size_resampled = int(len(X_resampled)*0.1)\n",
    "\n",
    "# train_ds_resampled = ds_resampled.take(train_size_resampled).batch(20)\n",
    "# val_ds_resampled = ds_resampled.skip(train_size_resampled).take(val_size_resampled).batch(20)\n",
    "# test_ds_resampled = ds_resampled.skip(train_size_resampled + val_size_resampled).take(test_size_resampled).batch(20)\n",
    "\n",
    "# model.fit(train_ds_resampled, validation_data=val_ds_resampled, epochs=EPOCH, verbose=1, callbacks=[tb_callback], class_weight=class_weights_dict)\n",
    "# # SMOTE 오버샘플링 추가 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8256/8256 [==============================] - 213s 25ms/step - loss: 2.2797 - accuracy: 0.7773 - val_loss: 0.6114 - val_accuracy: 0.7926\n",
      "Epoch 2/5\n",
      "8256/8256 [==============================] - 245s 30ms/step - loss: 2.0794 - accuracy: 0.7873 - val_loss: 0.5743 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "8256/8256 [==============================] - 269s 33ms/step - loss: 2.0103 - accuracy: 0.7936 - val_loss: 0.6453 - val_accuracy: 0.8199\n",
      "Epoch 4/5\n",
      "8256/8256 [==============================] - 311s 38ms/step - loss: 1.9684 - accuracy: 0.7953 - val_loss: 0.5695 - val_accuracy: 0.8158\n",
      "Epoch 5/5\n",
      "8256/8256 [==============================] - 317s 38ms/step - loss: 1.9282 - accuracy: 0.8015 - val_loss: 0.5630 - val_accuracy: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc1d3fc4550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 128,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 128,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128,\n",
    "               kernel_size = 5,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_sample_weight(class_weight='balanced', y=encoded_intents)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(np.unique(class_weights))}\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1, callbacks=[tb_callback], class_weight=class_weights_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 11s 9ms/step\n",
      "Precision: 0.7591\n",
      "Recall: 0.7034\n",
      "F1 Score: 0.7167\n",
      "Accuracy: 0.7034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_ds, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest loss:\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest accuracy:\u001b[39m\u001b[39m'\u001b[39m, accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_ds, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save('/home/azureuser/projects/aerius/ai/myapp/chatbot/models/intent_model_03_after_class_weight.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?     주문      0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?     주문      0\n",
      "2                     택배비 따로 추가되나요?     배송      1\n",
      "3                          택배비 있나요?     배송      1\n",
      "4                        택배비 따로 들어요     배송      1\n",
      "len of queries =  176625\n",
      "len of intents =  176625\n"
     ]
    }
   ],
   "source": [
    "filename='230814_intent_labeled_by_7_under_sampling'\n",
    "\n",
    "train_file = f'/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/{filename}.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial class distribution:\n",
      " label\n",
      "6    62199\n",
      "4    37356\n",
      "1    35110\n",
      "3    20243\n",
      "5    17249\n",
      "2     3894\n",
      "0      574\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = data['label'].value_counts()\n",
    "print(\"Initial class distribution:\\n\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Undersampled class distribution:\n",
      " label\n",
      "6    574\n",
      "4    574\n",
      "1    574\n",
      "3    574\n",
      "5    574\n",
      "2    574\n",
      "0    574\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "min_class_size = class_distribution.min()\n",
    "\n",
    "undersampled_data_list = [data[data['label'] == class_label].sample(min_class_size, random_state=42) \n",
    "                         for class_label in class_distribution.index]\n",
    "\n",
    "undersampled_data = pd.concat(undersampled_data_list)\n",
    "\n",
    "undersampled_class_distribution = undersampled_data['label'].value_counts()\n",
    "print(\"\\nUndersampled class distribution:\\n\", undersampled_class_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          query intent  label\n",
      "97549      옷이 사진과 너무 다른 거 아닌가요?  제품_기타      6\n",
      "6587        나이트슬립 종류별로 살 수 있나요?  제품_기타      6\n",
      "176207     목걸이에다가 제 이름 각인해 주세요.  제품_기타      6\n",
      "85364   지금 품절된 이 자켓은 언제쯤 들어올까요?  제품_기타      6\n",
      "61397       베이직 보드복 카키색 재입고됩니까?  제품_기타      6\n",
      "len of queries =  4018\n",
      "len of intents =  4018\n"
     ]
    }
   ],
   "source": [
    "queries = undersampled_data['query'].tolist()\n",
    "intents = undersampled_data['label'].tolist()\n",
    "\n",
    "print(undersampled_data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 7\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess()\n",
    "\n",
    "words = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        word_list, _ = p.divide_words_tags(preprocessed)\n",
    "        words.extend(word_list)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Convert the queries into sequences\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "141/141 [==============================] - 4s 18ms/step - loss: 1.9376 - accuracy: 0.1671 - val_loss: 1.8811 - val_accuracy: 0.2590\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 1.8207 - accuracy: 0.2283 - val_loss: 1.6589 - val_accuracy: 0.3885\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 1.5971 - accuracy: 0.3841 - val_loss: 1.4109 - val_accuracy: 0.4695\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 1s 9ms/step - loss: 1.4366 - accuracy: 0.4591 - val_loss: 1.2666 - val_accuracy: 0.5405\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 2s 12ms/step - loss: 1.3137 - accuracy: 0.5185 - val_loss: 1.0933 - val_accuracy: 0.6164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2d8c567550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 64,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 64,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "\n",
    "concat = concatenate([pool1, pool2])\n",
    "\n",
    "hidden = Dense(64, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 1s 28ms/step\n",
      "Precision: 0.1443\n",
      "Recall: 0.1471\n",
      "F1 Score: 0.1396\n",
      "Accuracy: 0.1471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2314119338989258\n",
      "Test accuracy: 0.5735660791397095\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step - loss: 1.2345 - accuracy: 0.5761\n",
      "Accuracy: 57.605987787246704\n",
      "loss: 1.2345458269119263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projects/aerius/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/{filename}_lower_dense.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 08:35:14.874692: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 08:35:17.342757: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 08:35:17.352830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 08:35:21.183149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              query intent  label\n",
      "0  아침도 아니고 밤 12시 30분에 결제한 건데도 그런가요?     주문      0\n",
      "1             실수로 취소하면 재주문해야 하는 거죠?     주문      0\n",
      "2                     택배비 따로 추가되나요?     배송      1\n",
      "3                          택배비 있나요?     배송      1\n",
      "4                        택배비 따로 들어요     배송      1\n",
      "len of queries =  176625\n",
      "len of intents =  176625\n"
     ]
    }
   ],
   "source": [
    "filename='230814_intent_labeled_by_7'\n",
    "\n",
    "train_file = f'/home/azureuser/projects/aerius/ai/myapp/chatbot/datas/{filename}.csv'\n",
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['label'].tolist()\n",
    "\n",
    "print(data.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    43535\n",
       "4    43535\n",
       "6    43535\n",
       "5    43535\n",
       "3    43535\n",
       "2    43535\n",
       "0    43535\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data.sample(frac=0.7, random_state=42)\n",
    "temp_data = data.drop(train_data.index)\n",
    "val_data = temp_data.sample(frac=0.66, random_state=42)\n",
    "test_data = temp_data.drop(val_data.index)\n",
    "\n",
    "# Oversampling the minority classes in training data\n",
    "max_size = train_data['label'].value_counts().max()\n",
    "lst = [train_data]\n",
    "for class_index, group in train_data.groupby('label'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "train_data_oversampled = pd.concat(lst)\n",
    "\n",
    "# Checking the distribution after oversampling\n",
    "oversampled_distribution = train_data_oversampled['label'].value_counts()\n",
    "\n",
    "oversampled_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             query intent  label\n",
      "32942                     13일 주문했는데 언제쯤 받을 수 있을까요?     배송      1\n",
      "126485  방금 추천해 주신 헤어오일 향이 너무 좋은데, 무슨 향인지 알려 주시겠어요?  제품_정보      4\n",
      "154437       배송 준비 중인데 내일 도착 예정이라고 뜨네요 낼 받을 수 있나요?     배송      1\n",
      "65307                           105싸이즈가입고가 되긴 할까요?  제품_기타      6\n",
      "75189                            겨울왕국 그려진 아동복 있어요?  제품_재고      5\n",
      "len of queries =  304745\n",
      "len of intents =  304745\n"
     ]
    }
   ],
   "source": [
    "queries = train_data_oversampled['query'].tolist()\n",
    "intents = train_data_oversampled['label'].tolist()\n",
    "\n",
    "print(train_data_oversampled.head(5))\n",
    "print('len of queries = ', len(queries))\n",
    "print('len of intents = ', len(intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique labels is 7\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(intents)\n",
    "\n",
    "encoded_intents = encoder.transform(intents)\n",
    "num_classes = len(encoder.classes_)\n",
    "print(f\"The number of unique labels is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from ai.myapp.chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess()\n",
    "\n",
    "words = []\n",
    "for sentence in queries:\n",
    "    if isinstance(sentence, str):  # Only process if the sentence is a string\n",
    "        preprocessed = p.delete_intent_trash_tags(sentence=sentence)\n",
    "        word_list, _ = p.divide_words_tags(preprocessed)\n",
    "        words.extend(word_list)\n",
    "    else:\n",
    "        print(f\"Found non-string value: {sentence}\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "p.initialize_tokenizer(words)\n",
    "\n",
    "# Convert the queries into sequences\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    sequence = p.text_to_sequence(sentence)\n",
    "    sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10667/10667 [==============================] - 265s 24ms/step - loss: 1.1137 - accuracy: 0.5723 - val_loss: 0.8946 - val_accuracy: 0.6559\n",
      "Epoch 2/5\n",
      "10667/10667 [==============================] - 309s 29ms/step - loss: 0.9581 - accuracy: 0.6350 - val_loss: 0.8382 - val_accuracy: 0.6761\n",
      "Epoch 3/5\n",
      "10667/10667 [==============================] - 308s 29ms/step - loss: 0.9123 - accuracy: 0.6522 - val_loss: 0.8082 - val_accuracy: 0.6875\n",
      "Epoch 4/5\n",
      "10667/10667 [==============================] - 322s 30ms/step - loss: 0.8865 - accuracy: 0.6618 - val_loss: 0.7772 - val_accuracy: 0.6991\n",
      "Epoch 5/5\n",
      "10667/10667 [==============================] - 359s 34ms/step - loss: 0.8630 - accuracy: 0.6703 - val_loss: 0.7586 - val_accuracy: 0.7026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f936dae1250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai.backend.settings import INTENT_MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=INTENT_MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, encoded_intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.tokenizer.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(INTENT_MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=INTENT_MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters = 64,\n",
    "               kernel_size = 3,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 64,\n",
    "               kernel_size = 4,\n",
    "               padding = 'valid',\n",
    "               activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "\n",
    "concat = concatenate([pool1, pool2])\n",
    "\n",
    "hidden = Dense(64, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(num_classes, name='logits')(dropout_hidden)\n",
    "predictions = Dense(num_classes, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524/1524 [==============================] - 30s 16ms/step\n",
      "Precision: 0.1448\n",
      "Recall: 0.1448\n",
      "F1 Score: 0.1420\n",
      "Accuracy: 0.1448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 1. 모델 예측\n",
    "y_pred = model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# test_ds_resampled에서 라벨만 추출\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# 2. 성능 지표 계산\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7649316787719727\n",
      "Test accuracy: 0.700564444065094\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524/1524 [==============================] - 22s 14ms/step - loss: 0.7583 - accuracy: 0.7047\n",
      "Accuracy: 70.47318816184998\n",
      "loss: 0.7583162188529968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projects/aerius/venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100}')\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "model.save(f'/home/azureuser/projects/aerius/ai/myapp/chatbot/models/{filename}_lower_dense.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
